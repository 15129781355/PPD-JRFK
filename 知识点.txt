1.数据清洗
    1.1：对缺失值的多维度处理
        按列：对缺失值进行绘图
            ：缺失太高（97%）的2列进行drop（有可能填了的人信用更高，要根据实际情况来处理）
            ：缺失值一般（60%）的三列类别信息，进行填充-1，当作缺省类型，因为用户的信息有可能有些不愿意填
            ：缺失值少的（15%）用均值（数值型）或者众数（类别型）
    1.2：对离群点的剔除方法
        按行：对缺失值进行绘图
            ：根据缺省值，来找到了离群点（另外缺失值个数可以作为一个特征，来衡量用户信息的完善程度）
        按列：统计方差/标准差
            ：将类别型的转编码，计算数值型特征的方差，剔除方差变化很小的特征/pandas的describe
        按模型：
            ：使用xgboost取最重要的（20/40）个特征，统计每个样本在这20个特征上的缺失值个数，将缺失值大于10的样本点作为离群点。
    1.3：文本处理
        ：字符大小写转化（需要查看数据的，unique/value_counts）
        ：空格字符转换（字符串可能包含空格）
        ：城市名处理（重庆和重庆市）
2.特征工程（特征达到上限）
    2.1：特征处理：
        ：连续数值型，归一化，标准化，域的变换log，exp
        ：离散值，one-hot编码
        ：类别型，one-hot编码
        以下为特殊的处理：
        地理位置处理：（提升特征区分度）
            ：对于类别型取值比较少的，可以人工直接完成，对每个类别进行违约率统计，构建几个二值特征（one-hot）即可，保留有判别性的特征，其他的抛弃不要！
            ：对于类别取值比较多的，可以直接独热编码，得到高维度模型进行训练Xgboost，筛选出来重要性高的一些！
        按城市等级合并：（特征一定不能太细，不然可能学习不到东西的，可以通过聚类的方式，增加一些密集一点的维度）
            ：类别特征取值太多了，进行编码后，有些特征太细了，样本可能不足以支撑在这个维度获得特征，好的解决方式就是聚类！
            ：比如：一线城市合并为1，二线城市合并为2，三线城市合并为3 / 南方和北方城市！
        经纬度的引入：（提升特征区分度）
            ：还可以对城市用经纬度替换，类别变量变为数值型变量，因为可以更好的区分了，城市等级特征显然还有些不强！加入此特征：（提升会很大）
        城市特征的向量化：（需要有更多的附加信息）
            （会有百分位提升）
            ：UserInfo_2:将城市特征里的无缺省值的城市个数，取Log（也可取指数，是可以尝试的）然后等值离散化到6-10个区间，将所有城市离散化为一个6维向量，
              "100000"表示在第1区间
    2.2：领域特征
        地理位置差异特征：
            ：差异度：衡量六个维度的城市是否一样（有千分位的提升）
        成交时间的特征：
            ：按照业务进行一些离散化的处理。x轴是日期，y轴是每日的借贷量，蓝色是违约样本，绿色是不违约样本
            ：按照图来看，折现比较曲折，可以考虑进行离散化，比如每10天进行一个划分！
    2.3：组合特征
        ：利用Xgboost训练完成，找到top40，两两进行组合：可进行除得到7000特征，在利用xgboost对这7000个特征训练，然后排序，取出top500，加到原始特征。
            还可以乘法特性（取对数）：log(x*y)筛选出其中270多维，单模型又提高了！
    2.4：UpadeInfo
        ：提取用户修改信息的次数，修改信息时间到成交时间的跨度，每种信息的修改次数等等！
    2.5：LoginInfo
        ：提取用户登陆信息特征 ，登陆天数，平均登陆间隔，以及操作哪种类型的次数！
    2.6：排序特征
        ：对原始的190维数值型特征按数值从小到大进行排序，得到190维排序特征。对异常数据有较强的鲁棒性，使得模型更加稳定，降低过拟合风险！
3.特征选择
    3.1：根据Xgboost的重要度排序（最常用！）
    3.2：LR回归加L1正则进行挑选
    3.3：PCA进行特征提取，但是在实际在竞赛中效果往往不太好
4.类别不平衡处理
    4.1：加大样本的权重（代价敏感学习）
        ：增加样本的权重
    4.2：上采样（生成样本/对样本重复几次）
        ：因为有的数据是不方便加噪声去生成样本，所以可以对样本多重复采样几次 / 或者（SMOTE）
5.模型设计与优化
    5.1：模型融合
            两个思路：
                ：最简单的思想是bagging
                ：在他之上是blending（会加上权重）
            做的方法：
                ：LR直接用 AUC0.772
                ：XGBoost用了两个版本 要么earling_stoping（很容易过拟合）  要么bagging
                ：SVM直接用
                ：多模型的blending（利用Liner_Regression获取权重）

1.xgboost选取特征，需要树的个数大一些否则可能特征数选取不够会出现问题！（可以进行调参进行改进！）
    ：训练集中的几个特征对输出的结果有很强的预测性，那么这些特征会被每个决策树所应用，这样会导致树之间具有相关性，这样并不会减小模型的方差。
2.xgboost的base_score":[0.1,0.3,0.6,0.8]  must be in (0,1) for logistic loss'
3.一般最后处理特征后，别用pandas进行传入数据了，很容易出现index错误的问题！
4.调试先用小样本跑通整个流程在进行其他的工作！
5.发现进行特征筛选再加上特征组合，特征的能力特别强只会选取几个特征最终！AUC值都是1。。。。。
6.不进行特征组合，直接选出前100感觉特征不强，auc值才60多预测的概率值也比较不可信，但是训练效果好但是测试很不好，感觉过拟合了，考虑放特征！！！选前40！
        ：我晕了SVC的C参数真的是慢，我用的线性核竟然慢到这种程度！
        ：感觉特征不够强，在训练集上表现很好，在测试集上很差可能是类别太不均衡，模型学不到东西，就是训练数据的上采样在这个数据里学不到东西！
        ：必须直接加强特征，增强模型学习能力，直接特征组合（先选出前40，在把这前40特征两两组合，选出200个，放入原始特征中即可！！！！！！
        ：但是加强特征后感觉太强了，AUC全是1，不管训练还是测试集！！！额！！！