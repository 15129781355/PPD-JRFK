拍拍贷的金融风控，是根据用户的特征预测借贷用户是否会逾期的二分类问题。
数据量有3W，有大约400个维度的特征。
数据清洗，离群点（样本缺失），缺失值（太多，一般，比较少），字符串（格式统一），样本不均衡的处理
特征工程，类别型特征（类别多，不多），连续型特征（归一化），时间型特征（离散化），特征组合（在选取）
特征选择，直接使用模型进行选取
建立模型，LR（偏爱类别特征），SVM（图片分类强大，线性非线性可调），XGboost（提升树），模型融合（利用线性回归）。‘
模型评估，ROC（排序能力），PR（注重分值大小的能力）
问题：
：模型融合要保证模型的多样性，模型满足高方差低偏差。所以使用了这三个很不相似的模型进行融合。
：缺失值少，对连续特征是平均值/中位数，离散特征是众数。

项目概述：
数据：总数据量3W有400个维度的特征。
1.数据的清洗对离群点（样本缺失值多的扔掉），缺失值（缺的太多扔掉少的用众数，一般的用聚类拟合），字符串（大小写问题）情况进行了处理；
2.特征工程的处理主要是对连续值的标准化（强行拉到一个分布），
    离散值的编码（对多类别的xgboost选取和少类别的直接编码）以及对离散值特征的额外的提取特征（比如对城市信息进行了一线二线三线离散化分
    /，还对城市信息进行了经纬度的划分作为两列特征），
    还有对连续的时间特征进行离散化处理（发现了时间和未违约的人成正比），
    最后在进行了强特征之间的特征组合在进行筛选前100放入原始特征。
3.特征选择从原始特征最终选出了前top40（防止特征过多导致样本过拟合，不能泛化，数据比较少）
在此进行了样本分布不均衡的处理，简单的用了上采样的方式进行处理。
4.建模，用到了LR（类别特征多，优先考虑到了）和SVM（分类很有名的，甚至以前用于图像分类）和xgboost（提升树模型对gbdt进行了改进）三种模型。
5.模型评估，用到了roc曲线（排序能力好，忽略了大小）和PR曲线进行评估还有log损失。

问题：
离群点检测怎样做的？只要针对样本缺失属性进行判别的大于30个就删除。
缺失值怎么处理的？缺失值太多的直接抛弃掉，缺的少的用到了众数没有用平均数是因为会受到离群点的影响很大而众数则不会（比如说很小的值和很大的值）。
                  缺的一般的值用到了随机森林来进行拟合（用训练数据来训练模型然后来对缺失标签样本进行预测即可。）
字符串你怎么处理的？有些列维度存在字符串大小写问题，比如QQ，qq，Qq，就像这种的处理。
为甚麽要对连续值进行标准化？因为可以加快训练速度，数值上的加快，而且ML是对样本分布的一种学习，学的是一种规律，进行标准化可以看成是一种
                            非线性变换改变了样本原有分布变为标准正态分布，而且这样做可以减弱离群点的影响使模型更加鲁棒，学习一种未知分布
                            更加容易，而不是从一个未知的很复杂的分布去学习。（所以说只要标准化不影响结果，是可以进行的，特性很好）
离散值的编码你是怎样进行的？离散值的属性比较少几个维度的考虑直接编码，但是会有属性特别多的比如城市信息300多维度直接使用xgboost进行
                            特征排序筛选（用该特征构建树的次数越多，特征重要性越大），还有维度是几十个的省份信息就是每个省份违约人数的
                            占比选出前几个省份即可。
离散特征的额外构造特征处理？对城市信息进行一线二线三线划分，以及经纬度的划分。
时间特征的处理？把时间与违约人数和未违约人数画图，可以发现时间和没有违约人数呈正比增长，与违约无关系是平滑的曲线，可以对时间离散化一个月为单位
                而且数据的信息也只是一年的。
特征组合你是怎么处理的？为了防止直接暴力组合会维度爆炸，我用了xgboost选出前40个强特征，然后在进行特征组合，之后在对组合后的特征选出前
                        400放入到原始特征中。还有一种方法是用决策树模型从根节点到叶子节点的每条路径为一个特征组合也可以，但是没有用。
多项式组合特征具体是怎样做的？多项式组合我用了指数为2的组合，就是两个特征相乘系数之和不能大于2。a和b[1,a,b,ab,a2,b2]！
特征选择怎么处理的？还是用了xgboost进行特征选择选出前100用于训练。
采样为啥用上采样呢？样本比例大约是10：1，上采样用到了SMOTH生成样本的算法，对两个样本的连线上进行生成样本的一个过程。下采样也可以，
                    可以构建10个相同的模型分别对样本进行划分变为10份，在进行bagging也可行，考虑到样本也就3W所以没这样进行，而且变成复杂度
                    也比较大。代价敏感学习：直接确定样本的权重分布即可，更方便。
模型为甚麽使用了LR？因为类别特征偏多，很适合LR，首先模型简单，类别特征稀疏性好可以加快矩阵运算，稀疏性也引进了非线性给原本广义线性模型
                    的LR引入了非线性因子，加强了模型的表达。
模型为甚么使用了SVM？因为SVM对于类别型任务很出名，image net大赛在没有卷积神经网络出现之前，使用SVM加人工提取特征可以达到很好的效果，
                     而且SVM的模型可线性和非线性，通过核变换来进行，而且高斯核可以理论上甚至可以达到无穷维度的空间。
模型为甚麽使用了xgboost？提升树模型由于每一颗实际都在拟合残差所以效果好，但是特别容易过拟合，引入提升树可以减弱偏差，在通过模型
                         融合提高泛化能力，是一个很好的选择。而且xgb在原有gbdt基础上使用了二阶导让模型速度更快，还天然的有正则化性和
                         模型复杂度项，而且针对大规模数据还有近似的权值分位点（直方图）算法也加快了速度在几乎不损失精度的情况下，主要还是
                         二阶导，比一阶导往往可能收敛的要更好，因为一阶导只是在斜率方向下降最好，而不能在其他方向在观测了，二阶导可以
                         所以xgboost使用了。
xgboost的早停是怎样的？为了防止在测试集上过拟合，因为测试误差一般是会先下降后增加，早停就是为了找到这个最低点。在训练n次后测试误差没有降低
                       那么就会自动停止掉。
你的模型调参具体是怎摸做的？我是先粗调找到一个差不多的范围，然后在细调找到较忧的点。每一次只调一个参数，控制模型不变来试验不同参数
                              的作用能力，再来确定最好的，最终进行一个参数组合。
模型是怎摸融合的？我先用LR和SVM和xgboost预测出了一个概率，然后把特征给LR进行学习的给3个模型学习一个权重，然后根据权重来进行求和！就得到了期望！
模型评估为啥用了auc，pr，log？使用auc是因为能对概率进行排序体现了正负样本得分相对的大小，例如对两个正负样本它的值只代表
                              正样本排在负样本前面的概率是多少，而不关注实际有多大，所以只看他不行，
                              所以还用了pr和log来进行弥补，pr可以体现实际的样本的分如何，针对不同样本的话那么还是auc更加稳定
                              pr可能浮动比较大，再加入了log损失的不满足性来进行衡量，再来分析。


